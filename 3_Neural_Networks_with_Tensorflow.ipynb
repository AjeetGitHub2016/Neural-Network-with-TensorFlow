{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of of simple Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\mnist\n"
     ]
    }
   ],
   "source": [
    "# give path of files\n",
    "data_dir = 'datasets\\mnist'\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  label\n",
       "0    0.png      4\n",
       "1    1.png      9\n",
       "2    2.png      1\n",
       "3    3.png      7\n",
       "4    4.png      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'Test.csv'))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe0fd9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display sample image\n",
    "img_name = rng.choice(train.filename)\n",
    "filepath = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
    "\n",
    "img = imread(filepath, flatten = True)\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load whole dataset \n",
    "temp = []\n",
    "for img_name in train.filename:\n",
    "    image_path = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
    "    img = imread(image_path, flatten=True)\n",
    "    img = img.astype('float32')\n",
    "    temp.append(img)\n",
    "    \n",
    "train_x = np.stack(temp)\n",
    "\n",
    "temp = []\n",
    "for img_name in test.filename:\n",
    "    image_path = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
    "    img = imread(image_path, flatten=True)\n",
    "    img = img.astype('float32')\n",
    "    temp.append(img)\n",
    "    \n",
    "test_x = np.stack(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "split_size = int(train_x.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = train.label.values[:split_size], train.label.values[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name).ix[batch_mask, 'label'].values\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_num_units = 28*28\n",
    "hidden_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "# define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "### define weights and biases of the neural network \n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define neural network\n",
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to initialize variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 3.72324\n",
      "Epoch: 2 cost = 0.55932\n",
      "Epoch: 3 cost = 0.32360\n",
      "Epoch: 4 cost = 0.24259\n",
      "Epoch: 5 cost = 0.18176\n",
      "\n",
      "Training complete!\n",
      "Validation Accuracy: 0.911905\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # create initialized variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    ### for each epoch, do:\n",
    "    ###   for each batch, do:\n",
    "    ###     create pre-processed batch\n",
    "    ###     run optimizer by feeding batch\n",
    "    ###     find cost and reiterate to minimize\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print( \"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "    print( \"\\nTraining complete!\")\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print( \"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}))\n",
    "    \n",
    "    predict = tf.argmax(output_layer, 1)\n",
    "    pred = predict.eval({x: test_x.reshape(-1, input_num_units)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQyjxQyjxQ6j/AkJIV8QwKkAwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdaac198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_name = rng.choice(test.filename)\n",
    "filepath = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
    "\n",
    "img = imread(filepath, flatten=True)\n",
    "\n",
    "test_index = int(img_name.split('.')[0]) - 49000\n",
    "\n",
    "print( \"Prediction is: \", pred[test_index])\n",
    "\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an MLP with TFLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, input_num_units)\n",
    "val_x = val_x.reshape(-1, input_num_units)\n",
    "test_x = test_x.reshape(-1, input_num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x /= train_x.max()\n",
    "val_x /= val_x.max()\n",
    "test_x /= test_x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "train_y = dense_to_one_hot(train_y)\n",
    "val_y = dense_to_one_hot(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layer\n",
    "model = tflearn.fully_connected(input_layer, 50, activation='relu')\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "lin_reg = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.22810\u001b[0m\u001b[0m | time: 0.999s\n",
      "| Adam | epoch: 002 | loss: 0.22810 - acc: 0.9346 -- iter: 34176/34300\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.23130\u001b[0m\u001b[0m | time: 1.003s\n",
      "| Adam | epoch: 002 | loss: 0.23130 - acc: 0.9349 -- iter: 34300/34300\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = tflearn.DNN(lin_reg)\n",
    "model.fit(train_x, train_y, n_epoch=2, batch_size=128, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91748299326215477]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_x, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(lin_reg, tensorboard_verbose=3, tensorboard_dir='tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters to look out for in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel that, hyperparameter tuning is the hardest in neural network in comparison to any other machine learning algorithm. You would be insane to apply Grid Search, as there are numerous parameters when it comes to tuning a neural network.\n",
    "\n",
    "![](image_5.png)\n",
    "\n",
    "Some important parameters to look out for while optimizing neural networks are:\n",
    "\n",
    "* Type of architecture\n",
    "* Number of Layers\n",
    "* Number of Neurons in a layer\n",
    "* Regularization parameters\n",
    "* Learning Rate\n",
    "* Type of optimization / backpropagation technique to use\n",
    "* Dropout rate\n",
    "* Weight sharing\n",
    "\n",
    "Also, there may be many more hyperparameters depending on the type of architecture. For example, if you use a convolutional neural network, you would have to look at hyperparameters like convolutional filter size, pooling value, etc.\n",
    "\n",
    "The best way to pick good parameters is to understand your problem domain. Research the previously applied techniques on your data, and most importantly  ask experienced people for insights to the problem. It’s the only way you can try to ensure you get a “good enough” neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your hands dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 0: Train for more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.54715\u001b[0m\u001b[0m | time: 1.046s\n",
      "| Adam | epoch: 010 | loss: 0.54715 - acc: 0.8912 -- iter: 34176/34300\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.52327\u001b[0m\u001b[0m | time: 1.052s\n",
      "| Adam | epoch: 010 | loss: 0.52327 - acc: 0.8982 -- iter: 34300/34300\n",
      "--\n",
      "Validation accuracy is [0.91993197277289673]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layer\n",
    "model = tflearn.fully_connected(input_layer, 50, activation='relu')\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "model = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.05)\n",
    "\n",
    "# train model\n",
    "model = tflearn.DNN(model)\n",
    "model.fit(train_x, train_y, n_epoch=10, batch_size=128, show_metric=True)\n",
    "print('Validation accuracy is', model.evaluate(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Round 1: Increase the number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let’s start our tweaking! Lets change our model to be “wide”, i.e. increase the number of neurons in our hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.58672\u001b[0m\u001b[0m | time: 2.233s\n",
      "| Adam | epoch: 010 | loss: 0.58672 - acc: 0.8733 -- iter: 34176/34300\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.56278\u001b[0m\u001b[0m | time: 2.239s\n",
      "| Adam | epoch: 010 | loss: 0.56278 - acc: 0.8758 -- iter: 34300/34300\n",
      "--\n",
      "Validation accuracy is [0.90523809520565734]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layer\n",
    "model = tflearn.fully_connected(input_layer, 500, activation='relu')\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "model = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.05)\n",
    "\n",
    "# train model\n",
    "model = tflearn.DNN(model)\n",
    "model.fit(train_x, train_y, n_epoch=10, batch_size=128, show_metric=True)\n",
    "print('Validation accuracy is', model.evaluate(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Round 2: Increase the number of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now instead of “wide”, we try making our model “deep”. We add four more hidden layers with 50 neurons each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.85119\u001b[0m\u001b[0m | time: 1.710s\n",
      "| Adam | epoch: 010 | loss: 0.85119 - acc: 0.7971 -- iter: 34176/34300\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.86786\u001b[0m\u001b[0m | time: 1.717s\n",
      "| Adam | epoch: 010 | loss: 0.86786 - acc: 0.7955 -- iter: 34300/34300\n",
      "--\n",
      "Validation accuracy is [0.82741496585664298]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layers\n",
    "model = tflearn.fully_connected(input_layer, 50, activation='relu')\n",
    "model = tflearn.fully_connected(model, 50, activation='relu')\n",
    "model = tflearn.fully_connected(model, 50, activation='relu')\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "model = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.05)\n",
    "\n",
    "# train model\n",
    "model = tflearn.DNN(model)\n",
    "model.fit(train_x, train_y, n_epoch=10, batch_size=128, show_metric=True)\n",
    "print('Validation accuracy is', model.evaluate(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3: Add regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we didn’t get what we expected. This may be because our model may be overfitting. To deal with this, we use a method called dropout. Dropout is essentially randomly turning off parts of the model so that it does not “overlearn” a concept (To read more about dropout, check out the article on core concepts of neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m1.39336\u001b[0m\u001b[0m | time: 1.081s\n",
      "| Adam | epoch: 010 | loss: 1.39336 - acc: 0.5103 -- iter: 34176/34300\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m1.36953\u001b[0m\u001b[0m | time: 1.085s\n",
      "| Adam | epoch: 010 | loss: 1.36953 - acc: 0.5139 -- iter: 34300/34300\n",
      "--\n",
      "Validation accuracy is [0.61843537403612725]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layers\n",
    "model = tflearn.fully_connected(input_layer, 50, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "model = tflearn.fully_connected(model, 50, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "model = tflearn.fully_connected(model, 50, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "model = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.05)\n",
    "\n",
    "# train model\n",
    "model = tflearn.DNN(model)\n",
    "model.fit(train_x, train_y, n_epoch=10, batch_size=128, show_metric=True)\n",
    "print('Validation accuracy is', model.evaluate(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 4: Try everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try another thing, we make our model both deep and wide! We should implement all the tweaks that we learnt before. For the purpose of getting faster results, you can reduce the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 26799  | total loss: \u001b[1m\u001b[32m0.00999\u001b[0m\u001b[0m | time: 1.522s\n",
      "| Adam | epoch: 100 | loss: 0.00999 - acc: 0.9987 -- iter: 34176/34300\n",
      "Training Step: 26800  | total loss: \u001b[1m\u001b[32m0.00934\u001b[0m\u001b[0m | time: 1.528s\n",
      "| Adam | epoch: 100 | loss: 0.00934 - acc: 0.9989 -- iter: 34300/34300\n",
      "--\n",
      "Validation accuracy is [0.97285714267873435]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# create model\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "# dense layers\n",
    "model = tflearn.fully_connected(input_layer, 100, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "model = tflearn.fully_connected(model, 70, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "model = tflearn.fully_connected(model, 30, activation='relu')\n",
    "model = tflearn.dropout(model, 0.9)\n",
    "\n",
    "# Fully Connected Layer\n",
    "model = tflearn.fully_connected(model, 10, activation='softmax')\n",
    "model = tflearn.regression(model, optimizer='adam', loss='categorical_crossentropy',learning_rate=0.001)\n",
    "\n",
    "# train model\n",
    "model = tflearn.DNN(model)\n",
    "model.fit(train_x, train_y, n_epoch=100, batch_size=128, show_metric=True)\n",
    "print('Validation accuracy is', model.evaluate(val_x, val_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
